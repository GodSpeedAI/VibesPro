"""
End-to-end tests for {{ project_name }} prompt optimizer.

Tests the complete workflow from CLI to domain logic,
ensuring all components work together correctly.
"""

import pytest
import asyncio
import tempfile
import subprocess
import json
from pathlib import Path
from {{ package_path }} import AnalyzePromptUseCase, TiktokenAdapter
{% if database_type == "sled" -%}
from {{ package_path }} import SledTemporalDatabaseAdapter
{%- endif %}
from {{ package_path }}.domain.entities.prompt import Prompt
from {{ package_path }}.domain.value_objects.ai_model import AIModel


class TestEndToEndWorkflow:
    """End-to-end test suite for the complete prompt optimizer workflow."""
    
    @pytest.fixture
    def temp_prompt_file(self, sample_prompt_content):
        """Create a temporary prompt file for CLI testing."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(sample_prompt_content)
            temp_path = f.name
        
        yield temp_path
        
        # Cleanup
        Path(temp_path).unlink(missing_ok=True)
    
    @pytest.fixture
    def temp_database_path(self):
        """Create a temporary database path for testing."""
        temp_dir = tempfile.mkdtemp()
        db_path = Path(temp_dir) / "test_e2e_db"
        yield str(db_path)
        
        # Cleanup
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    {% if include_cli_tools -%}
    def test_cli_analyze_command_basic(self, temp_prompt_file):
        """Test basic CLI analyze command functionality."""
        # Arrange
        script_path = Path(__file__).parent.parent / "measure_tokens_enhanced.py"
        
        if not script_path.exists():
            pytest.skip("CLI script not available in test environment")
        
        # Act
        result = subprocess.run([
            "python3", str(script_path), temp_prompt_file, "analyze", "--format", "json"
        ], capture_output=True, text=True, timeout=30)
        
        # Assert
        assert result.returncode == 0, f"CLI failed with error: {result.stderr}"
        
        # Parse JSON output
        output_data = json.loads(result.stdout)
        assert "token_count" in output_data
        assert "effectiveness_score" in output_data
        assert output_data["token_count"]["total_tokens"] > 0
    
    def test_cli_optimize_command_basic(self, temp_prompt_file):
        """Test basic CLI optimize command functionality."""
        # Arrange
        script_path = Path(__file__).parent.parent / "measure_tokens_enhanced.py"
        
        if not script_path.exists():
            pytest.skip("CLI script not available in test environment")
        
        # Act
        result = subprocess.run([
            "python3", str(script_path), temp_prompt_file, "optimize", 
            "--goal", "clarity", "--format", "json"
        ], capture_output=True, text=True, timeout=30)
        
        # Assert
        assert result.returncode == 0, f"CLI failed with error: {result.stderr}"
        
        # Parse JSON output
        output_data = json.loads(result.stdout)
        assert "optimized_prompt" in output_data
        assert "improvement_metrics" in output_data
        assert len(output_data["optimized_prompt"]) > 0
    
    def test_shell_script_integration(self, temp_prompt_file):
        """Test shell script wrapper integration."""
        # Arrange
        script_path = Path(__file__).parent.parent / "measure_tokens.sh"
        
        if not script_path.exists():
            pytest.skip("Shell script not available in test environment")
        
        # Make sure script is executable
        import os
        os.chmod(script_path, 0o755)
        
        # Act
        result = subprocess.run([
            str(script_path), temp_prompt_file, "--analyze", "--format", "json"
        ], capture_output=True, text=True, timeout=30)
        
        # Assert
        assert result.returncode == 0, f"Shell script failed: {result.stderr}"
        assert "token_count" in result.stdout or "Token Analysis" in result.stdout
    {%- endif %}
    
    @pytest.mark.asyncio
    async def test_complete_analysis_workflow(self, temp_database_path, sample_prompt_content):
        """Test the complete analysis workflow using the library directly."""
        # Arrange
        token_counter = TiktokenAdapter()
        {% if database_type == "sled" -%}
        database = SledTemporalDatabaseAdapter(temp_database_path)
        {%- else -%}
        # Mock database for other types
        from unittest.mock import AsyncMock, Mock
        database = Mock()
        database.store_analysis = AsyncMock()
        database.get_historical_patterns = AsyncMock(return_value=[])
        {%- endif %}
        
        use_case = AnalyzePromptUseCase(token_counter, database)
        prompt = Prompt.create(content=sample_prompt_content)
        
        # Act
        result = await use_case.execute(prompt, AIModel.GPT4)
        
        # Assert
        assert result is not None
        assert result.prompt == prompt
        assert result.token_count.count > 0
        assert result.effectiveness_score.overall_score >= 0
        assert result.effectiveness_score.overall_score <= 100
        assert isinstance(result.optimization_suggestions, list)
    
    @pytest.mark.asyncio
    async def test_optimization_workflow_improves_metrics(self, temp_database_path, sample_prompt_content):
        """Test that optimization workflow actually improves prompt metrics."""
        # Arrange
        from {{ package_path }}.application.use_cases.optimize_prompt_use_case import OptimizePromptUseCase
        from {{ package_path }}.domain.value_objects.optimization_goal import OptimizationGoal
        
        token_counter = TiktokenAdapter()
        {% if database_type == "sled" -%}
        database = SledTemporalDatabaseAdapter(temp_database_path)
        {%- else -%}
        # Mock database for other types
        from unittest.mock import AsyncMock, Mock
        database = Mock()
        database.store_analysis = AsyncMock()
        database.store_optimization = AsyncMock()
        database.get_historical_patterns = AsyncMock(return_value=[])
        {%- endif %}
        
        optimize_use_case = OptimizePromptUseCase(token_counter, database)
        
        # Create a deliberately verbose prompt for optimization
        verbose_prompt = Prompt.create(content="""
        Please, if you would be so kind, analyze the following code implementation 
        and provide comprehensive suggestions for improvement, taking into consideration 
        all aspects of software engineering best practices, including but not limited to 
        performance optimization, code readability, maintainability, and overall quality.
        """.strip())
        
        # Act
        result = await optimize_use_case.execute(
            verbose_prompt, 
            OptimizationGoal.CONCISENESS, 
            AIModel.GPT4
        )
        
        # Assert
        assert result is not None
        assert result.optimized_prompt.content != verbose_prompt.content
        
        # The optimized prompt should be more concise
        original_length = len(verbose_prompt.content)
        optimized_length = len(result.optimized_prompt.content)
        assert optimized_length < original_length, "Optimized prompt should be shorter"
        
        # Token count should be reduced for conciseness goal
        original_tokens = result.original_analysis.token_count.count
        optimized_tokens = result.optimized_analysis.token_count.count
        assert optimized_tokens <= original_tokens, "Token count should not increase"
    
    @pytest.mark.asyncio
    async def test_temporal_learning_workflow(self, temp_database_path):
        """Test that the system learns from repeated usage patterns."""
        {% if database_type == "sled" -%}
        # Arrange
        token_counter = TiktokenAdapter()
        database = SledTemporalDatabaseAdapter(temp_database_path)
        use_case = AnalyzePromptUseCase(token_counter, database)
        
        # Create similar prompts
        prompts = [
            Prompt.create(content="Analyze this code for performance issues"),
            Prompt.create(content="Review this code for performance problems"),
            Prompt.create(content="Examine this code for performance bottlenecks")
        ]
        
        # Act - Analyze multiple similar prompts
        results = []
        for prompt in prompts:
            result = await use_case.execute(prompt, AIModel.GPT4)
            results.append(result)
        
        # Assert - The system should have stored analysis data
        # This is a basic test - in a real implementation, we'd verify
        # that subsequent analyses show learning from patterns
        assert len(results) == 3
        for result in results:
            assert result.effectiveness_score.overall_score > 0
        {%- else -%}
        pytest.skip("Temporal learning test requires sled database")
        {%- endif %}
    
    @pytest.mark.asyncio 
    async def test_different_optimization_goals_produce_different_results(self, temp_database_path):
        """Test that different optimization goals produce meaningfully different results."""
        # Arrange
        from {{ package_path }}.application.use_cases.optimize_prompt_use_case import OptimizePromptUseCase
        from {{ package_path }}.domain.value_objects.optimization_goal import OptimizationGoal
        
        token_counter = TiktokenAdapter()
        {% if database_type == "sled" -%}
        database = SledTemporalDatabaseAdapter(temp_database_path)
        {%- else -%}
        from unittest.mock import AsyncMock, Mock
        database = Mock()
        database.store_analysis = AsyncMock()
        database.store_optimization = AsyncMock()
        database.get_historical_patterns = AsyncMock(return_value=[])
        {%- endif %}
        
        optimize_use_case = OptimizePromptUseCase(token_counter, database)
        
        # Test prompt that can be optimized in different ways
        base_prompt = Prompt.create(content="""
        Could you please help me understand how to implement a function that processes data?
        I need something that works efficiently and is easy to understand.
        """.strip())
        
        goals = [
            OptimizationGoal.CLARITY,
            OptimizationGoal.CONCISENESS,
            OptimizationGoal.SPECIFICITY
        ]
        
        # Act
        results = {}
        for goal in goals:
            result = await optimize_use_case.execute(base_prompt, goal, AIModel.GPT4)
            results[goal] = result
        
        # Assert
        # Each optimization should produce different results
        optimized_contents = [result.optimized_prompt.content for result in results.values()]
        assert len(set(optimized_contents)) > 1, "Different goals should produce different optimizations"
        
        # Verify goal-specific improvements
        clarity_result = results[OptimizationGoal.CLARITY]
        conciseness_result = results[OptimizationGoal.CONCISENESS]
        
        # Conciseness optimization should generally produce shorter text
        clarity_length = len(clarity_result.optimized_prompt.content)
        conciseness_length = len(conciseness_result.optimized_prompt.content)
        
        # This is a heuristic - conciseness might not always be shorter,
        # but it should generally aim for efficiency
        assert "efficiency" in str(results).lower() or conciseness_length <= clarity_length * 1.2
    
    def test_error_handling_with_invalid_input(self):
        """Test that the system handles invalid input gracefully."""
        {% if include_cli_tools -%}
        # Test CLI with non-existent file
        script_path = Path(__file__).parent.parent / "measure_tokens_enhanced.py"
        
        if script_path.exists():
            result = subprocess.run([
                "python3", str(script_path), "nonexistent_file.txt", "analyze"
            ], capture_output=True, text=True)
            
            assert result.returncode != 0
            assert "does not exist" in result.stderr or "not found" in result.stderr.lower()
        {%- endif %}
    
    @pytest.mark.asyncio
    async def test_performance_with_large_prompt(self):
        """Test system performance with large prompts."""
        # Arrange
        large_content = "This is a performance test prompt. " * 100  # ~500 words
        large_prompt = Prompt.create(content=large_content)
        
        token_counter = TiktokenAdapter()
        
        # Act
        import time
        start_time = time.time()
        result = await token_counter.count_tokens(large_prompt.content, AIModel.GPT4)
        end_time = time.time()
        
        # Assert
        assert result.count > 0
        # Should complete within reasonable time (2 seconds for large prompt)
        assert (end_time - start_time) < 2.0, "Token counting should be performant"