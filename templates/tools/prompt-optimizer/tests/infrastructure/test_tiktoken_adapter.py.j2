"""
Integration tests for TiktokenAdapter following TDD principles.

Tests cover token counting accuracy, model compatibility,
and error handling for the tiktoken integration.
"""

import pytest
from {{ package_path }}.infrastructure.adapters.tiktoken_adapter import TiktokenAdapter
from {{ package_path }}.domain.value_objects.ai_model import AIModel


class TestTiktokenAdapter:
    """Test suite for TiktokenAdapter."""

    @pytest.fixture
    def adapter(self):
        """Create TiktokenAdapter instance for testing."""
        return TiktokenAdapter()

    @pytest.mark.asyncio
    async def test_count_tokens_basic_functionality(self, adapter):
        """Test basic token counting functionality."""
        # Arrange
        text = "Hello, world! This is a test prompt."
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        assert result.count > 0
        assert result.model == model
        assert result.estimated_cost > 0
        assert isinstance(result.count, int)
        assert isinstance(result.estimated_cost, float)

    @pytest.mark.asyncio
    async def test_count_tokens_different_models(self, adapter):
        """Test token counting with different AI models."""
        # Arrange
        text = "This is a test prompt for token counting."
        models = [AIModel.GPT4, AIModel.GPT4_TURBO, AIModel.GPT3_5_TURBO]

        # Act & Assert
        for model in models:
            result = await adapter.count_tokens(text, model)

            assert result.count > 0
            assert result.model == model
            # Token count should be consistent across models (same encoding)
            if models.index(model) > 0:
                previous_result = await adapter.count_tokens(text, models[0])
                assert result.count == previous_result.count  # Same text, same encoding

    @pytest.mark.asyncio
    async def test_count_tokens_empty_text(self, adapter):
        """Test token counting with empty text."""
        # Arrange
        text = ""
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        assert result.count == 0
        assert result.estimated_cost == 0.0
        assert result.model == model

    @pytest.mark.asyncio
    async def test_count_tokens_whitespace_only(self, adapter):
        """Test token counting with whitespace-only text."""
        # Arrange
        text = "   \n\t  "
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        # Whitespace typically counts as tokens
        assert result.count >= 0
        assert result.model == model

    @pytest.mark.asyncio
    async def test_count_tokens_very_long_text(self, adapter):
        """Test token counting with very long text."""
        # Arrange
        text = "word " * 1000  # 1000 repetitions
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        assert result.count > 500  # Should be substantial
        assert result.estimated_cost > 0
        assert result.model == model

    @pytest.mark.asyncio
    async def test_count_tokens_special_characters(self, adapter):
        """Test token counting with special characters and emojis."""
        # Arrange
        text = "Hello! ðŸŒŸ Special chars: @#$%^&*() ä¸­æ–‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        assert result.count > 0
        assert result.model == model
        assert result.estimated_cost > 0

    @pytest.mark.asyncio
    async def test_count_tokens_cost_calculation_accuracy(self, adapter):
        """Test that cost calculation matches expected values."""
        # Arrange
        text = "This is exactly ten tokens for testing cost calculation."
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        expected_cost = result.count * model.cost_per_token
        assert abs(result.estimated_cost - expected_cost) < 0.000001  # Float precision

    @pytest.mark.asyncio
    async def test_count_tokens_breakdown_information(self, adapter):
        """Test that token count includes useful breakdown information."""
        # Arrange
        text = "Test prompt with multiple sentences. This helps analyze breakdown."
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(text, model)

        # Assert
        assert result.breakdown is not None
        assert isinstance(result.breakdown, dict)
        # Should contain useful information about the tokenization
        if result.breakdown:
            assert "encoding_name" in result.breakdown or "model_name" in result.breakdown

    @pytest.mark.asyncio
    async def test_count_tokens_consistent_results(self, adapter):
        """Test that token counting produces consistent results."""
        # Arrange
        text = "This prompt should produce consistent token counts."
        model = AIModel.GPT4

        # Act - Count the same text multiple times
        results = []
        for _ in range(3):
            result = await adapter.count_tokens(text, model)
            results.append(result)

        # Assert - All results should be identical
        for result in results[1:]:
            assert result.count == results[0].count
            assert result.estimated_cost == results[0].estimated_cost
            assert result.model == results[0].model

    @pytest.mark.asyncio
    async def test_count_tokens_with_code_content(self, adapter):
        """Test token counting with code content."""
        # Arrange
        code_text = '''
        def hello_world():
            print("Hello, World!")
            return True

        if __name__ == "__main__":
            hello_world()
        '''
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(code_text, model)

        # Assert
        assert result.count > 0
        assert result.model == model
        # Code typically has higher token density
        assert result.count > len(code_text.split()) * 0.5  # Rough heuristic

    @pytest.mark.asyncio
    async def test_count_tokens_with_markdown_content(self, adapter):
        """Test token counting with markdown content."""
        # Arrange
        markdown_text = '''
        # Header

        This is **bold** text and *italic* text.

        - List item 1
        - List item 2

        ```python
        print("code block")
        ```

        [Link](https://example.com)
        '''
        model = AIModel.GPT4

        # Act
        result = await adapter.count_tokens(markdown_text, model)

        # Assert
        assert result.count > 0
        assert result.model == model
        # Markdown syntax should add to token count
        plain_text = "Header This is bold text and italic text List item 1 List item 2 print code block Link"
        plain_result = await adapter.count_tokens(plain_text, model)
        assert result.count >= plain_result.count  # Markdown should be >= plain text

    @pytest.mark.asyncio
    async def test_token_count_comparison_across_models(self, adapter):
        """Test token count consistency across different models with same encoding."""
        # Arrange
        text = "Consistent tokenization test across different model configurations."
        gpt4_models = [AIModel.GPT4, AIModel.GPT4_TURBO, AIModel.GPT3_5_TURBO]

        # Act
        results = {}
        for model in gpt4_models:
            result = await adapter.count_tokens(text, model)
            results[model] = result

        # Assert
        # Models with same encoding should have same token count
        token_counts = [result.count for result in results.values()]
        assert all(count == token_counts[0] for count in token_counts), \
            "Token counts should be consistent across models with same encoding"

        # But costs should differ based on model pricing
        costs = [result.estimated_cost for result in results.values()]
        assert len(set(costs)) > 1, "Costs should differ across models"
