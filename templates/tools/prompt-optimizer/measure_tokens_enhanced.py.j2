#!/usr/bin/env python3
"""
{{ project_name }} - Enhanced Token Counter CLI with ML-Powered Optimization
Generated from VibePro prompt-optimizer template

This CLI provides accurate token counting and ML-powered prompt optimization
using the {{ project_name }} prompt optimizer system.
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, Any, Optional

# Add the project root to Python path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from libs.prompt_optimizer.application.use_cases.analyze_prompt_use_case import AnalyzePromptUseCase
    from libs.prompt_optimizer.application.use_cases.optimize_prompt_use_case import OptimizePromptUseCase
    from libs.prompt_optimizer.infrastructure.adapters.tiktoken_adapter import TiktokenAdapter
    from libs.prompt_optimizer.infrastructure.adapters.sled_temporal_database_adapter import SledTemporalDatabaseAdapter
    from libs.prompt_optimizer.domain.entities.prompt import Prompt
    from libs.prompt_optimizer.domain.value_objects.optimization_goal import OptimizationGoal
    from libs.prompt_optimizer.domain.value_objects.ai_model import AIModel
    ENHANCED_MODE = True
except ImportError as e:
    print(f"Warning: Enhanced mode not available: {e}", file=sys.stderr)
    ENHANCED_MODE = False


class PromptOptimizerCLI:
    """CLI interface for {{ project_name }} prompt optimization system."""
    
    def __init__(self):
        """Initialize the CLI with necessary adapters."""
        if ENHANCED_MODE:
            self.token_counter = TiktokenAdapter()
            self.database = SledTemporalDatabaseAdapter(str(project_root / "temporal_db"))
            self.analyze_use_case = AnalyzePromptUseCase(self.token_counter, self.database)
            self.optimize_use_case = OptimizePromptUseCase(self.token_counter, self.database)
    
    def _print_formatted(self, data: Dict[str, Any], format_type: str = "human") -> None:
        """Print data in specified format."""
        if format_type == "json":
            print(json.dumps(data, indent=2, default=str))
        else:
            self._print_human_readable(data)
    
    def _print_human_readable(self, data: Dict[str, Any]) -> None:
        """Print analysis data in human-readable format."""
        print(f"\nüîç {{ project_name }} - Prompt Analysis Results")
        print("=" * 50)
        
        if "token_count" in data:
            tc = data["token_count"]
            print(f"üìä Token Analysis:")
            print(f"   ‚Ä¢ Total tokens: {tc.get('total_tokens', 'N/A')}")
            print(f"   ‚Ä¢ Model: {tc.get('model', 'N/A')}")
            print(f"   ‚Ä¢ Estimated cost: ${tc.get('estimated_cost', 0.0):.4f}")
        
        if "effectiveness_score" in data:
            es = data["effectiveness_score"]
            print(f"\nüéØ Effectiveness Analysis:")
            print(f"   ‚Ä¢ Overall score: {es.get('overall_score', 'N/A')}/100")
            print(f"   ‚Ä¢ Clarity: {es.get('clarity_score', 'N/A')}/100")
            print(f"   ‚Ä¢ Conciseness: {es.get('conciseness_score', 'N/A')}/100")
            print(f"   ‚Ä¢ Specificity: {es.get('specificity_score', 'N/A')}/100")
        
        if "optimization_suggestions" in data:
            suggestions = data["optimization_suggestions"]
            if suggestions:
                print(f"\nüí° Optimization Suggestions:")
                for i, suggestion in enumerate(suggestions, 1):
                    print(f"   {i}. {suggestion}")
        
        if "optimized_prompt" in data:
            print(f"\n‚ú® Optimized Prompt:")
            print("-" * 30)
            print(data["optimized_prompt"])
            print("-" * 30)
        
        print()
    
    async def analyze_prompt(self, prompt_text: str, model: str = "gpt-4", format_type: str = "human") -> None:
        """Analyze a prompt and display results."""
        if not ENHANCED_MODE:
            self._fallback_analysis(prompt_text, format_type)
            return
        
        try:
            prompt = Prompt.create(content=prompt_text)
            ai_model = AIModel.from_string(model)
            
            result = await self.analyze_use_case.execute(prompt, ai_model)
            
            analysis_data = {
                "prompt": prompt_text[:100] + "..." if len(prompt_text) > 100 else prompt_text,
                "token_count": {
                    "total_tokens": result.token_count.count,
                    "model": result.token_count.model.value,
                    "estimated_cost": result.token_count.estimated_cost
                },
                "effectiveness_score": {
                    "overall_score": result.effectiveness_score.overall_score,
                    "clarity_score": result.effectiveness_score.clarity_score,
                    "conciseness_score": result.effectiveness_score.conciseness_score,
                    "specificity_score": result.effectiveness_score.specificity_score
                },
                "optimization_suggestions": result.optimization_suggestions
            }
            
            self._print_formatted(analysis_data, format_type)
            
        except Exception as e:
            print(f"Error during analysis: {e}", file=sys.stderr)
            sys.exit(1)
    
    async def optimize_prompt(self, prompt_text: str, goal: str = "effectiveness", 
                            model: str = "gpt-4", format_type: str = "human") -> None:
        """Optimize a prompt and display results."""
        if not ENHANCED_MODE:
            print("Error: Optimization requires enhanced mode dependencies", file=sys.stderr)
            sys.exit(1)
        
        try:
            prompt = Prompt.create(content=prompt_text)
            optimization_goal = OptimizationGoal.from_string(goal)
            ai_model = AIModel.from_string(model)
            
            result = await self.optimize_use_case.execute(prompt, optimization_goal, ai_model)
            
            optimization_data = {
                "original_prompt": prompt_text,
                "optimized_prompt": result.optimized_prompt.content,
                "goal": goal,
                "improvement_metrics": {
                    "token_reduction": result.improvement_metrics.get("token_reduction", 0),
                    "effectiveness_improvement": result.improvement_metrics.get("effectiveness_improvement", 0),
                    "clarity_improvement": result.improvement_metrics.get("clarity_improvement", 0)
                },
                "token_count": {
                    "original_tokens": result.original_analysis.token_count.count,
                    "optimized_tokens": result.optimized_analysis.token_count.count,
                    "savings": result.original_analysis.token_count.count - result.optimized_analysis.token_count.count
                }
            }
            
            self._print_formatted(optimization_data, format_type)
            
        except Exception as e:
            print(f"Error during optimization: {e}", file=sys.stderr)
            sys.exit(1)
    
    def _fallback_analysis(self, prompt_text: str, format_type: str = "human") -> None:
        """Provide basic analysis when enhanced mode is unavailable."""
        word_count = len(prompt_text.split())
        char_count = len(prompt_text)
        estimated_tokens = int(word_count * 4 / 3)  # Rough estimation
        
        fallback_data = {
            "prompt": prompt_text[:100] + "..." if len(prompt_text) > 100 else prompt_text,
            "fallback_analysis": {
                "word_count": word_count,
                "character_count": char_count,
                "estimated_tokens": estimated_tokens,
                "note": "Basic analysis - install dependencies for enhanced features"
            }
        }
        
        if format_type == "json":
            print(json.dumps(fallback_data, indent=2))
        else:
            print(f"\nüìù {{ project_name }} - Basic Prompt Analysis")
            print("=" * 50)
            print(f"üìä Word count: {word_count}")
            print(f"üìä Character count: {char_count}")
            print(f"ü§ñ Estimated tokens: {estimated_tokens}")
            print(f"\nüí° Install Python dependencies for ML-powered analysis and optimization")
            print()


def show_help():
    """Display help information."""
    help_text = """
{{ project_name }} - Enhanced Token Counter CLI

USAGE:
    python measure_tokens_enhanced.py <prompt-file> <command> [OPTIONS]

COMMANDS:
    analyze                     Perform detailed prompt analysis (default)
    optimize                    Optimize the prompt for effectiveness

OPTIONS:
    --goal <goal>              Optimization goal: clarity|conciseness|effectiveness|token_efficiency
    --model <model>            AI model: gpt-4|gpt-4-turbo|gpt-3.5-turbo|claude-3-opus|claude-3-sonnet
    --format <format>          Output format: human|json
    --help                     Show this help message

EXAMPLES:
    # Analyze a prompt
    python measure_tokens_enhanced.py my_prompt.txt analyze

    # Optimize for clarity
    python measure_tokens_enhanced.py my_prompt.txt optimize --goal clarity

    # Get JSON output
    python measure_tokens_enhanced.py my_prompt.txt analyze --format json

FEATURES:
    ‚Ä¢ Accurate token counting using tiktoken
    ‚Ä¢ ML-powered effectiveness scoring
    ‚Ä¢ Prompt optimization with multiple goals
    ‚Ä¢ Cost estimation for different models
    ‚Ä¢ Temporal learning from usage patterns
    ‚Ä¢ Integration with {{ project_name }} AI context system

Generated with VibePro prompt-optimizer template
"""
    print(help_text)


async def main():
    """Main CLI entry point."""
    if len(sys.argv) < 2 or "--help" in sys.argv:
        show_help()
        return
    
    # Parse arguments
    prompt_file = sys.argv[1]
    command = "analyze"  # default
    goal = "effectiveness"
    model = "gpt-4"
    format_type = "human"
    
    # Check if prompt file exists
    if not Path(prompt_file).exists():
        print(f"Error: Prompt file '{prompt_file}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    # Parse command and options
    i = 2
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg in ["analyze", "optimize"]:
            command = arg
        elif arg == "--goal" and i + 1 < len(sys.argv):
            goal = sys.argv[i + 1]
            i += 1
        elif arg == "--model" and i + 1 < len(sys.argv):
            model = sys.argv[i + 1]
            i += 1
        elif arg == "--format" and i + 1 < len(sys.argv):
            format_type = sys.argv[i + 1]
            i += 1
        
        i += 1
    
    # Read prompt file
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            prompt_text = f.read().strip()
    except Exception as e:
        print(f"Error reading prompt file: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Initialize CLI and execute command
    cli = PromptOptimizerCLI()
    
    try:
        if command == "analyze":
            await cli.analyze_prompt(prompt_text, model, format_type)
        elif command == "optimize":
            await cli.optimize_prompt(prompt_text, goal, model, format_type)
        else:
            print(f"Unknown command: {command}", file=sys.stderr)
            sys.exit(1)
    except KeyboardInterrupt:
        print("\nOperation cancelled by user", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())